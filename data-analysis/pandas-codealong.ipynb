{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "While NumPy can be used to import data, it is optimized around numerical data. Many data sets include categorical variables. For these data sets, it is best to use a library called `pandas`, which focuses on creating and manipulating data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "With `pandas` imported, we can read in .csv files with the `pandas` function `read_csv()`.\n",
    "\n",
    "In that function, we can specify the file we want to use with a URL or with the path to a local file as a string.\n",
    "\n",
    "This saves the data in a structure called a DataFrame.\n",
    "\n",
    "We are going to be using data on [long term average precipitation and temperature values in Boston from ~1980s-2010 from NOAA](https://www.ncei.noaa.gov/data/normals-monthly/doc/NORMAL_MLY_documentation.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"\"\n",
    "# filename = \"https://raw.githubusercontent.com/ENVS110a-SP23/python/main/data/boston_precip_temp.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now saved as a data frame in Python as the variable `df`. With the data now in the environment, we can take a look at the first few rows with `df.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this data frame has several different columns, with information about stations, precipitation and temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an excel file you can also `pd.read_excel()`. You can specify the sheet name, as well. The default is the first sheet, and you can provide either a single sheet name, or a list of sheets you want as an alternative, which gives you a dictionary of pandas DataFrames.\n",
    "\n",
    "If you say `sheet_name=None`, you will get all of the sheets back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"\"\n",
    "\n",
    "\n",
    "# xlsx = \"https://raw.githubusercontent.com/ENVS110a-SP23/python/main/data/boston_precip_temp.xlsx\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_excel(xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(xlsx, sheet_name=['Sheet1','Sheet2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure data is in correct form\n",
    "\n",
    "When the data does not have the standard format, there can be issues. This tends to happen when the first line of the .csv file is not column names.\n",
    "\n",
    "For an example, we'll take a look at [a data set of two files on arctic vegetation plots](http://dx.doi.org/10.3334/ORNLDAAC/1358)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environmental_data = data_dir + \"Arrigetch_Peaks_Environmental_Data_raw.csv\"\n",
    "environmental_data = \"https://raw.githubusercontent.com/ENVS110a-SP23/python/main/data/Arrigetch_Peaks_Environmental_Data_raw.csv\"\n",
    "pd.read_csv(environmental_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_file = data_dir + \"/Arrigetch_Peaks_Species_Data_raw.csv\"\n",
    "species_file = \"/Users/fordfishman/GitHub/envs110/python/data/Arrigetch_Peaks_Species_Data_raw.csv\"\n",
    "\n",
    "pd.read_csv(species_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "For in-class questions, we'll be working with a data set called Gapminder. It is in the `data` subdirectory in this repo as `gapminder.csv`. You can also find it at this stable url: `https://raw.githubusercontent.com/ENVS110a-SP23/python/main/data/gapminder.csv`.\n",
    "\n",
    "Load this data set and display the first few rows with `.head()`. **Make sure to save it as a different variable name than `df` to make sure you don't overwrite the precipitation and temperature data frame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize data frame\n",
    "\n",
    "It is important to understand the data we are working with before we begin analysis. First, let's look at the dimenions of the data frame using `df.shape`. It gives the number of rows by the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that our data frame has 300 rows by 7 columns.\n",
    "\n",
    "We can get out those numbers individually through indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`len(df)` also gets back how many rows you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `df.columns` to display the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming columns and rows\n",
    "\n",
    "We can rename as many columns as you want with `df.rename(columns = {old_name:new_name,...})`. \n",
    "\n",
    "Note that you need to re-assign to `df` or make a new variable if you want to save the renamed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also re-assign row names by saying `index` instead of `columns`. This is more rare, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "Using the gapminder data frame, print out the column names. Rename the `age5_surviving` and `babies_per_woman` columns to be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables\n",
    "Next, let's summarize the categorical, non-numerical variables. For instance, we can identify how many unique regions we have in the data set.\n",
    "\n",
    "First, to select a column, we use the notation `df['COLUMN_NAME']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your column name, you can also refer to the column with `df.column_name` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify unique entries in this column, we can use the `pd.unique()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also just use the `len()` function to see how many unique values we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical columns can be summarized in several ways. Let's find the mean first.\n",
    "\n",
    "To make things simpler, we'll just do calculations on the `population`, `life_expectancy`, and `babies_per_woman` columns. We can put those names in a `list` and then specify that list for the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [ 'date', 'temp', 'diurnal_temp_range', 'precip-total','snow-totals' ] # numerical columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this set of columns, we can run `.mean()` to find the mean of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a larger variety of summary statistics, we can use the `.describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also break down subgroupings of our data with the method `.groupby()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Using the gapminder data, use `.groupby()` to get summary statistics by region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing rows and specific entries\n",
    "\n",
    "You can also to access a specific row using `df.loc[ROW, :]`. The colon specifies to select all columns for that row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `.loc` to find the value of specific entries, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we multiply a data frame by a single number, each value in the column will be muliplied by that value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn this into a new column by assigning to `df['new_col_name']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy functions work very well with numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new column is now reflected in the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do math between columns, since they have the same length. Elements of the same row are added, substacted, multiplied, or divided. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your own data frame\n",
    "\n",
    "To make your own data frame without a .csv, we use the function `pd.DataFrame()`. There are many ways to use this function to construct a data frame. \n",
    "\n",
    "Here, we show how to convert a dictionary of lists into a data frame. Each list will be its own column, and you need to make sure the lists are all the same length. The keys of each list should be the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'a': [1, 3, 5],\n",
    "    'b': ['apple', 'banana', 'apple'],\n",
    "    'c': [-2., -3., -5.]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use lists of lists or 2D NumPy arrays to create data frames. Each list will be a row, instead of a column, and you will need to specify the column name as another argument in `pd.DataFrame()` called `columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [\n",
    "    [1, 'apple', -2.],\n",
    "    [3, 'banana', -3.],\n",
    "    [5, 'apple', -5.]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we need to save this as a variable to use it in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data frame as .csv\n",
    "\n",
    "If you have made modifications to a data set in Python and want to export that to a new .csv, you can easily do that with the `.to_csv()` method that all pandas data frames have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.DataFrame(data_list, columns=['a', 'b', 'c'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Putting it together\n",
    "\n",
    "In assignment 2, we moved information gathered from some researchers into a nested data structure. Instead, transfer these data into a Pandas dataframe. Display the data frame, and export it as a .csv file.\n",
    "\n",
    "As a reminder, each list is in the same order as the researchers name -> all of Haley McCann's data is at index `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "researchers = ['Haley McCann', 'Siena Welch', 'Jaylin Mercado', 'Ismael Hayden', 'Nina Bright']\n",
    "\n",
    "temperatures = [29.75, 12.63, 31.58, 7.16, 32.51]\n",
    "\n",
    "populations = [442, 336, 505, 913, 933]\n",
    "\n",
    "dates = ['5/25/2022','3/18/2022','6/28/2022','11/11/2022','7/6/2023']\n",
    "\n",
    "### Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Pandas docs](https://pandas.pydata.org/docs/)\n",
    "- [Pandas getting started](https://pandas.pydata.org/docs/getting_started/index.html#getting-started)\n",
    "- [Pandas cheatsheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "- [PySpark for big data](https://spark.apache.org/docs/latest/api/python/)\n",
    "\n",
    "This lesson is adapted from \n",
    "[Software Carpentry](http://swcarpentry.github.io/python-novice-gapminder/design/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b52b534d3e8b2df8a7f375ea8b488ed7c2e546a5d77230d70bd2c6badbd8c861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
