{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "Let's keep going through how to use Pandas for processing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dir = \"/Users/fordfishman/GitHub/envs110/python/data/\"\n",
    "\n",
    "csv = data_dir + \"boston_precip_temp.csv\"\n",
    "\n",
    "df = pd.read_csv(csv) # read in data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset by row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we want to create a subset of the main data frame based on certain conditions. We do this by using `df.loc` and specifying a condition for the rows. \n",
    "\n",
    "Below, we take all of the rows where `temp` is greater or equal to 50 with `df['temp'] >= temp` and assign this to a new data frame.\n",
    "\n",
    "To check that this was done correctly, we can look at the minimum of the `temp` column in the new data frame with `.min()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also subset with categorical variables. Here, we take all rows where the station name is `NORWOOD MEMORIAL AIRPORT, MA US`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NORWOOD MEMORIAL AIRPORT, MA US'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NORWOOD MEMORIAL AIRPORT, MA US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, we can use subsetting to remove rows with improper values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can provide multiple conditions at the same time as well, but we have to use different operators instead of `and` and `or`. Instead we use `&` and `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "What is the mean temperaure recorded across all stations in the month of July (7th month) when total precipitation is greater than 3.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed when we do these subsets, the row numbers actually do not reset. We have to manually do this with `.reset_index()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.loc[(df['name']=='NORWOOD MEMORIAL AIRPORT, MA US') | (df['temp']>=50), :]\n",
    "\n",
    "# reset index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not want to save the old row numbers, you can the argument `drop=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditions are one way to limit our data set to proper and non-missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Below is a dictionary where the keys are station names, and the values are the zip codes for the station names. Creating a new data frame, replace the values in the `name` column with their corresponding zip codes.\n",
    "\n",
    "*Hint - Use a for loop.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_codes = {\n",
    "    'BEDFORD HANSCOM FIELD, MA US': '01731',\n",
    "    'BEVERLY MUNICIPAL AIRPORT, MA US': '01915',\n",
    "    'BEVERLY, MA US': '01915',\n",
    "    'BLUE HILL LCD, MA US': '02191',\n",
    "    'BOSTON, MA US': '02108',\n",
    "    'BRIDGEWATER, MA US': '02324',\n",
    "    'BROCKTON, MA US': '02301',\n",
    "    'FRANKLIN, MA US': '02038',\n",
    "    'GROVELAND, MA US': '01834',\n",
    "    'HAVERHILL, MA US': '01830',\n",
    "    'HINGHAM, MA US': '02043',\n",
    "    'JAMAICA PLAIN, MA US': '02130',\n",
    "    'LAWRENCE MUNICIPAL AIRPORT, MA US': '01841',\n",
    "    'LAWRENCE, MA US': '01841',\n",
    "    'LOWELL, MA US': '01852',\n",
    "    'MARBLEHEAD, MA US': '01945',\n",
    "    'MAYNARD, MA US': '01754',\n",
    "    'MIDDLETON, MA US': '01949',\n",
    "    'MILFORD, MA US': '01757',\n",
    "    'NATICK, MA US': '01760',\n",
    "    'NORTON, MA US': '02766',\n",
    "    'NORWOOD MEMORIAL AIRPORT, MA US': '02062',\n",
    "    'READING, MA US': '01867',\n",
    "    'SOUTH WEYMOUTH NAS, MA US': '02190',\n",
    "    'WALPOLE 2, MA US': '02081',\n",
    "}\n",
    "\n",
    "# your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1      NaN\n",
       "2      NaN\n",
       "3      NaN\n",
       "4      NaN\n",
       "      ... \n",
       "295    0.0\n",
       "296    0.0\n",
       "297    NaN\n",
       "298    1.5\n",
       "299    9.7\n",
       "Name: snow-totals, Length: 300, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['snow-totals'].replace(-7777., np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any and all\n",
    "\n",
    "`any()` and `all()` are functions built into Pandas data frames and columns. They let us check if any of the values are `True`, or of all the values are `True`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these functions to see if any rows or columns are missing any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all values in data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many rows are missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `.any()` to get back a data frame with only rows with missing values, or (more helpfully), only rows without missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only not missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sort the rows in a data frame by the value of a column, we can use the `.sort_values()` method. The argument `by` requires a list with a column name. \n",
    "\n",
    "Again, if you want to use the sorted version in the future, you need to save it as a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>apple</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>banana</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a       b    c\n",
       "0  1   apple -2.0\n",
       "2  5   apple -5.0\n",
       "1  3  banana -3.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = {\n",
    "    'a': [1, 3, 5],\n",
    "    'b': ['apple', 'banana', 'apple'],\n",
    "    'c': [-2., -3., -5.]\n",
    "}\n",
    "\n",
    "my_df = pd.DataFrame(my_dict)\n",
    "\n",
    "# sort by column b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also sort descending by specifying the `ascending=False` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort ascending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desired, multiple column names can be specified, with priority given to those first in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi column sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add rows\n",
    "There are multiple ways to add a new row to a data frame. The most straightforward way is to use the `pandas.concat()` function with a new data frame with just one row. \n",
    "\n",
    "We put the the two data frames into a list, and we set `axis=0` to make sure it adds as a row. We will specify `.reset_index(drop=True)` to reset row numbers to account for the new row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use this approach to add multiple rows, as well, by having the new data frame consist of multiple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Add a new row to the Boston precipitation and temperature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join data frames\n",
    "A critical tool in data wrangling is combining data frames that share common values, columns, or identifiers.\n",
    "\n",
    "Let's important two new .csv files and join them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   record_id  month  day  year  plot_id species_id sex  hindfoot_length  \\\n",
      "0          1      7   16  1977        2         NL   M             32.0   \n",
      "1          2      7   16  1977        3         NL   M             33.0   \n",
      "2          3      7   16  1977        2         DM   F             37.0   \n",
      "3          4      7   16  1977        7         DM   M             36.0   \n",
      "4          5      7   16  1977        3         DM   M             35.0   \n",
      "\n",
      "   weight  \n",
      "0     NaN  \n",
      "1     NaN  \n",
      "2     NaN  \n",
      "3     NaN  \n",
      "4     NaN  \n",
      "  species_id             genus          species    taxa\n",
      "0         AB        Amphispiza        bilineata    Bird\n",
      "1         AH  Ammospermophilus          harrisi  Rodent\n",
      "2         AS        Ammodramus       savannarum    Bird\n",
      "3         BA           Baiomys          taylori  Rodent\n",
      "4         CB   Campylorhynchus  brunneicapillus    Bird\n"
     ]
    }
   ],
   "source": [
    "surveys_df = pd.read_csv(data_dir+\"surveys.csv\", keep_default_na=False, na_values=[\"\"])\n",
    "species_df = pd.read_csv(data_dir+\"species.csv\", keep_default_na=False, na_values=[\"\"])\n",
    "\n",
    "print(surveys_df.head())\n",
    "print(species_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shared column between these data frames is `species_id`, so this is the column we will want to join around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner Join\n",
    "The pandas function for performing joins is called `merge()` and an Inner join is the default option.\n",
    "\n",
    "Inner joins take all rows from both data frames that share values from an identifier column. In our case, this means that our joined data frame will only include rows with species identifiers present in `species_df` and `surveys_df`.\n",
    "\n",
    "<img src=\"https://datacarpentry.org/python-ecology-lesson/fig/inner-join.png\" alt=\"inner join\" width=250px>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In this case `species_id` is the only column name in  both dataframes, so if we skipped `left_on`\n",
    "# And `right_on` arguments we would still get the same result\n",
    "\n",
    "# What's the size of the output data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result `merged_inner` data frame contains all of the columns from `surveys_df` (`record_id`, `month`, `day`, etc.) as well as all the columns from `species_df` (`species_id`, `genus`, `species`, and `taxa`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left join\n",
    "\n",
    "What if we want to add information from `species_df` to `surveys_df`without losing any of the information from `surveys_df`? In this case, we use a different type of join called a left join, where we keep all rows from the data frame we call left (in our case `surveys_df`) and only take rows from the right data frame (`species_df`) with species IDs in `surveys_df`.\n",
    "\n",
    "<img src=\"https://datacarpentry.org/python-ecology-lesson/fig/left-join.png\" alt=\"left join\" width=250px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A left join is performed in pandas by calling the same `merge()` function used for inner join, but using the `how='left'` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Compare the two types of joins. Which type of join results in more rows with missing data? Can you think of situations where one type of join might be more useful than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with dates\n",
    "\n",
    "In the Boston precipitation and temperature data, time is labeled as a number standing for month. This is relatively straight forward to handle.\n",
    "\n",
    "Often, data sheets contain dates in formats that you will need to process before using. For instance, a single column might contain year, month, day and time of day information.\n",
    "\n",
    "Here, we use data on net emissions by country, which has severl columns with dates and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso3_country</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>original_inventory_sector</th>\n",
       "      <th>gas</th>\n",
       "      <th>emissions_quantity</th>\n",
       "      <th>emissions_quantity_units</th>\n",
       "      <th>temporal_granularity</th>\n",
       "      <th>created_date</th>\n",
       "      <th>modified_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>2021-12-31 00:00:00</td>\n",
       "      <td>net-grassland-emissions</td>\n",
       "      <td>co2</td>\n",
       "      <td>4.035921e+06</td>\n",
       "      <td>tonnes</td>\n",
       "      <td>annual</td>\n",
       "      <td>2022-10-10 17:13:05.115568</td>\n",
       "      <td>2022-10-24 16:24:04.711642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>2021-12-31 00:00:00</td>\n",
       "      <td>net-grassland-emissions</td>\n",
       "      <td>co2e_100yr</td>\n",
       "      <td>4.035921e+06</td>\n",
       "      <td>tonnes</td>\n",
       "      <td>annual</td>\n",
       "      <td>2022-10-10 17:13:05.115599</td>\n",
       "      <td>2022-10-24 16:24:04.711642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>2021-12-31 00:00:00</td>\n",
       "      <td>net-grassland-emissions</td>\n",
       "      <td>co2e_20yr</td>\n",
       "      <td>4.035921e+06</td>\n",
       "      <td>tonnes</td>\n",
       "      <td>annual</td>\n",
       "      <td>2022-10-10 17:13:05.115604</td>\n",
       "      <td>2022-10-24 16:24:04.711642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGO</td>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>2021-12-31 00:00:00</td>\n",
       "      <td>net-grassland-emissions</td>\n",
       "      <td>co2</td>\n",
       "      <td>-1.348266e+08</td>\n",
       "      <td>tonnes</td>\n",
       "      <td>annual</td>\n",
       "      <td>2022-10-10 17:13:05.115709</td>\n",
       "      <td>2022-10-24 16:24:04.711642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGO</td>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>2021-12-31 00:00:00</td>\n",
       "      <td>net-grassland-emissions</td>\n",
       "      <td>co2e_100yr</td>\n",
       "      <td>-1.348266e+08</td>\n",
       "      <td>tonnes</td>\n",
       "      <td>annual</td>\n",
       "      <td>2022-10-10 17:13:05.115727</td>\n",
       "      <td>2022-10-24 16:24:04.711642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iso3_country           start_time             end_time  \\\n",
       "0          AFG  2021-01-01 00:00:00  2021-12-31 00:00:00   \n",
       "1          AFG  2021-01-01 00:00:00  2021-12-31 00:00:00   \n",
       "2          AFG  2021-01-01 00:00:00  2021-12-31 00:00:00   \n",
       "3          AGO  2021-01-01 00:00:00  2021-12-31 00:00:00   \n",
       "4          AGO  2021-01-01 00:00:00  2021-12-31 00:00:00   \n",
       "\n",
       "  original_inventory_sector         gas  emissions_quantity  \\\n",
       "0   net-grassland-emissions         co2        4.035921e+06   \n",
       "1   net-grassland-emissions  co2e_100yr        4.035921e+06   \n",
       "2   net-grassland-emissions   co2e_20yr        4.035921e+06   \n",
       "3   net-grassland-emissions         co2       -1.348266e+08   \n",
       "4   net-grassland-emissions  co2e_100yr       -1.348266e+08   \n",
       "\n",
       "  emissions_quantity_units temporal_granularity                created_date  \\\n",
       "0                   tonnes               annual  2022-10-10 17:13:05.115568   \n",
       "1                   tonnes               annual  2022-10-10 17:13:05.115599   \n",
       "2                   tonnes               annual  2022-10-10 17:13:05.115604   \n",
       "3                   tonnes               annual  2022-10-10 17:13:05.115709   \n",
       "4                   tonnes               annual  2022-10-10 17:13:05.115727   \n",
       "\n",
       "                modified_date  \n",
       "0  2022-10-24 16:24:04.711642  \n",
       "1  2022-10-24 16:24:04.711642  \n",
       "2  2022-10-24 16:24:04.711642  \n",
       "3  2022-10-24 16:24:04.711642  \n",
       "4  2022-10-24 16:24:04.711642  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emissions_path = data_dir + \"country_net-grassland-emissions.csv\"\n",
    "\n",
    "emissions = pd.read_csv(emissions_path)\n",
    "emissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns with dates are all objects. That is to say, right now the dates are all strings, which can be difficult to work with at times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iso3_country                  object\n",
       "start_time                    object\n",
       "end_time                      object\n",
       "original_inventory_sector     object\n",
       "gas                           object\n",
       "emissions_quantity           float64\n",
       "emissions_quantity_units      object\n",
       "temporal_granularity          object\n",
       "created_date                  object\n",
       "modified_date                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the column into a more useful data type, we can use the pandas function `pd.to_datetime()` to convert to a `datatype` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_datetime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_datetime()` makes some assumptions about order of month and year, so it can be a good idea to tell it the format we are working with the `format` parameter, which takes a string.\n",
    "\n",
    "We can use identifiers to match up the components of date and time:\n",
    "-`'%Y'` year (4 digits) or `'%y'` year (2 digits)\n",
    "-`'%m'` month\n",
    "-`'%d'` day\n",
    "-`'%H'` hour\n",
    "-`'%M'` minute\n",
    "-`'%S'` second\n",
    "\n",
    "You'll want to check the order these components are found in your column, as well as if there are characters between them.\n",
    "\n",
    "For us, the year comes first, followed by month and then day. Then comes hour, minute, and second. The date components are separated by dashes, the time components have colons, and there is a space between date and time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2021-01-01\n",
       "1      2021-01-01\n",
       "2      2021-01-01\n",
       "3      2021-01-01\n",
       "4      2021-01-01\n",
       "          ...    \n",
       "8745   2015-01-01\n",
       "8746   2015-01-01\n",
       "8747   2015-01-01\n",
       "8748   2015-01-01\n",
       "8749   2015-01-01\n",
       "Name: start_time, Length: 8750, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then reassign the date column to be a useful data type.\n",
    "\n",
    "Once we do that, we can pull out parts of the date as we need them using `.dt.strftime()`, specifying the component(s) of the date we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions['start_time'] = pd.to_datetime(emissions.start_time, format=\"%Y-%m-%d %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can very specifically ask for different formats back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions['start_time'].dt.strftime(  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Using the emissions data set and the `end_time` and `emissions_quantity` columns, print out the mean `emissions_quantity` for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b52b534d3e8b2df8a7f375ea8b488ed7c2e546a5d77230d70bd2c6badbd8c861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
